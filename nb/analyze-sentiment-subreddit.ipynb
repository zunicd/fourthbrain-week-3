{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p align = \"center\" draggable=‚Äùfalse‚Äù ><img src=\"https://user-images.githubusercontent.com/37101144/161836199-fdb0219d-0361-4988-bf26-48b0fad160a3.png\" \n",
    "     width=\"200px\"\n",
    "     height=\"auto\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1 align=\"center\" id=\"heading\">Sentiment Analysis of Reddit Data using Reddit API</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this live coding session, we leverage the Python Reddit API Wrapper (`PRAW`) to retrieve data from subreddits on [Reddit](https://www.reddit.com), and perform sentiment analysis using [`pipelines`](https://huggingface.co/docs/transformers/main_classes/pipelines) from [HuggingFace ( ü§ó the GitHub of Machine Learning )](https://techcrunch.com/2022/05/09/hugging-face-reaches-2-billion-valuation-to-build-the-github-of-machine-learning/), powered by [transformer](https://arxiv.org/pdf/1706.03762.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the end of the session, you will "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- know how to work with APIs\n",
    "- feel more comfortable navigating thru documentation, even inspecting the source code\n",
    "- understand what a `pipeline` object is in HuggingFace\n",
    "- perform sentiment analysis using `pipeline`\n",
    "- run a python script in command line and get the results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## How to Submit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- At the end of each task, commit* the work into the repository you created before the assignment\n",
    "- After completing all three tasks, make sure to push the notebook containing all code blocks and output cells to your repository you created before the assignment\n",
    "- Submit the link to the notebook in Canvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\\***NEVER** commit a notebook displaying errors unless it is instructed otherwise. However, commit often; recall git ABC = **A**lways **B**e **C**ommitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Task I: Instantiate a Reddit API Object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The first task is to instantiate a Reddit API object using [PRAW](https://praw.readthedocs.io/en/stable/), through which you will retrieve data. PRAW is a wrapper for [Reddit API](https://www.reddit.com/dev/api) that makes interacting with the Reddit API easier unless you are already an expert of [`requests`](https://docs.python-requests.org/en/latest/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1. Install packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Please ensure you've ran all the cells in the `imports.ipynb`, located [here](https://github.com/FourthBrain/MLE-8/blob/main/assignments/week-3-analyze-sentiment-subreddit/imports.ipynb), to make sure you have all the required packages for today's assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "####  2. Create a new app on Reddit "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Create a new app on Reddit and save secret tokens; refer to [post in medium](https://towardsdatascience.com/how-to-use-the-reddit-api-in-python-5e05ddfd1e5c) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Create a Reddit account if you don't have one, log into your account.\n",
    "- To access the API, we need create an app. Slight updates, on the website, you need to navigate to `preference` > `app`, or click [this link](https://www.reddit.com/prefs/apps) and scroll all the way down. \n",
    "- Click to create a new app, fill in the **name**, choose `script`, fill in  **description** and **redirect uri** ( The redirect URI is where the user is sent after they've granted OAuth access to your application (more info [here](https://github.com/reddit-archive/reddit/wiki/OAuth2)) For our purpose, you can enter some random url, e.g., www.google.com; as shown below.\n",
    "\n",
    "\n",
    "    <img src=\"https://miro.medium.com/max/700/1*lRBvxpIe8J2nZYJ6ucMgHA.png\" width=\"500\"/>\n",
    "- Jot down `client_id` (left upper corner) and `client_secret` \n",
    "\n",
    "    NOTE: CLIENT_ID refers to 'personal use script\" and CLIENT_SECRET to secret.\n",
    "    \n",
    "    <div>\n",
    "    <img src=\"https://miro.medium.com/max/700/1*7cGAKth1PMrEf2sHcQWPoA.png\" width=\"300\"/>\n",
    "    </div>\n",
    "\n",
    "- Create `secrets_reddit.py` in the same directory with this notebook, fill in `client_id` and `secret_id` obtained from the last step. We will need to import those constants in the next step.\n",
    "    ```\n",
    "    REDDIT_API_CLIENT_ID = \"client_id\"\n",
    "    REDDIT_API_CLIENT_SECRET = \"secret_id\"\n",
    "    REDDIT_API_USER_AGENT = \"any string except bot; ex. My User Agent\"\n",
    "    ```\n",
    "- Add `secrets_reddit.py` to your `.gitignore` file if not already done. NEVER push credentials to a repo, private or public. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3. Instantiate a `Reddit` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now you are ready to create a read-only `Reddit` instance. Refer to [documentation](https://praw.readthedocs.io/en/stable/code_overview/reddit_instance.html) when necessary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T13:20:03.905649Z",
     "iopub.status.busy": "2022-07-14T13:20:03.904558Z",
     "iopub.status.idle": "2022-07-14T13:20:04.224846Z",
     "shell.execute_reply": "2022-07-14T13:20:04.224197Z",
     "shell.execute_reply.started": "2022-07-14T13:20:03.905547Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "import secrets_reddit\n",
    "\n",
    "# Create a Reddit object which allows us to interact with the Reddit API\n",
    "reddit = praw.Reddit(\n",
    "    # YOUR CODE HERE\n",
    "    client_id = secrets_reddit.REDDIT_API_CLIENT_ID,\n",
    "    client_secret = secrets_reddit.REDDIT_API_CLIENT_SECRET,\n",
    "    user_agent = secrets_reddit.REDDIT_API_USER_AGENT\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T13:20:04.226140Z",
     "iopub.status.busy": "2022-07-14T13:20:04.225948Z",
     "iopub.status.idle": "2022-07-14T13:20:04.232182Z",
     "shell.execute_reply": "2022-07-14T13:20:04.231221Z",
     "shell.execute_reply.started": "2022-07-14T13:20:04.226122Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<praw.reddit.Reddit object at 0x7fb8d07c3940>\n"
     ]
    }
   ],
   "source": [
    "print(reddit) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>   \n",
    "\n",
    "```<praw.reddit.Reddit object at 0x10f8a0ac0>```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 4. Instantiate a `subreddit` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Lastly, create a `subreddit` object for your favorite subreddit and inspect the object. The expected outputs you will see are from `r/machinelearning` unless otherwise specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T13:20:04.233140Z",
     "iopub.status.busy": "2022-07-14T13:20:04.232928Z",
     "iopub.status.idle": "2022-07-14T13:20:04.238831Z",
     "shell.execute_reply": "2022-07-14T13:20:04.238081Z",
     "shell.execute_reply.started": "2022-07-14T13:20:04.233121Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "subreddit = reddit.subreddit(\"machinelearning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What is the display name of the subreddit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T13:20:04.240645Z",
     "iopub.status.busy": "2022-07-14T13:20:04.240373Z",
     "iopub.status.idle": "2022-07-14T13:20:04.246802Z",
     "shell.execute_reply": "2022-07-14T13:20:04.245771Z",
     "shell.execute_reply.started": "2022-07-14T13:20:04.240623Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "machinelearning\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print(subreddit.display_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>   \n",
    "\n",
    "    machinelearning\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How about its title, is it different from the display name?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T13:20:04.247742Z",
     "iopub.status.busy": "2022-07-14T13:20:04.247554Z",
     "iopub.status.idle": "2022-07-14T13:20:04.709394Z",
     "shell.execute_reply": "2022-07-14T13:20:04.708597Z",
     "shell.execute_reply.started": "2022-07-14T13:20:04.247724Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Machine Learning\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print(subreddit.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>   \n",
    "\n",
    "    Machine Learning\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Print out the description of the subreddit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T13:20:04.710315Z",
     "iopub.status.busy": "2022-07-14T13:20:04.710104Z",
     "iopub.status.idle": "2022-07-14T13:20:04.715714Z",
     "shell.execute_reply": "2022-07-14T13:20:04.714624Z",
     "shell.execute_reply.started": "2022-07-14T13:20:04.710294Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**[Rules For Posts](https://www.reddit.com/r/MachineLearning/about/rules/)**\n",
      "--------\n",
      "+[Research](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AResearch)\n",
      "--------\n",
      "+[Discussion](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ADiscussion)\n",
      "--------\n",
      "+[Project](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AProject)\n",
      "--------\n",
      "+[News](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print(subreddit.description[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Expected output:</summary>\n",
    "\n",
    "    **[Rules For Posts](https://www.reddit.com/r/MachineLearning/about/rules/)**\n",
    "    --------\n",
    "    +[Research](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AResearch)\n",
    "    --------\n",
    "    +[Discussion](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3ADiscussion)\n",
    "    --------\n",
    "    +[Project](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict_sr=on&q=flair%3AProject)\n",
    "    --------\n",
    "    +[News](https://www.reddit.com/r/MachineLearning/search?sort=new&restrict\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Task II: Parse comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 1. Top Posts of All Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Find titles of top 10 posts of **all time** from your favorite subreddit. Refer to [Obtain Submission Instances from a Subreddit Section](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html)) if necessary. Verify if the titles match what you read on Reddit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T13:20:04.717019Z",
     "iopub.status.busy": "2022-07-14T13:20:04.716699Z",
     "iopub.status.idle": "2022-07-14T13:20:04.766291Z",
     "shell.execute_reply": "2022-07-14T13:20:04.765421Z",
     "shell.execute_reply.started": "2022-07-14T13:20:04.716991Z"
    },
    "hidden": true,
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0msubreddit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mtime_filter\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'all'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mgenerator_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mIterator\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Return a :class:`.ListingGenerator` for top items.\n",
       "\n",
       ":param time_filter: Can be one of: ``\"all\"``, ``\"day\"``, ``\"hour\"``,\n",
       "    ``\"month\"``, ``\"week\"``, or ``\"year\"`` (default: ``\"all\"``).\n",
       "\n",
       ":raises: :py:class:`ValueError` if ``time_filter`` is invalid.\n",
       "\n",
       "Additional keyword arguments are passed in the initialization of\n",
       ":class:`.ListingGenerator`.\n",
       "\n",
       "This method can be used like:\n",
       "\n",
       ".. code-block:: python\n",
       "\n",
       "    reddit.domain(\"imgur.com\").top(time_filter=\"week\")\n",
       "    reddit.multireddit(redditor=\"samuraisam\", name=\"programming\").top(time_filter=\"day\")\n",
       "    reddit.redditor(\"spez\").top(time_filter=\"month\")\n",
       "    reddit.redditor(\"spez\").comments.top(time_filter=\"year\")\n",
       "    reddit.redditor(\"spez\").submissions.top(time_filter=\"all\")\n",
       "    reddit.subreddit(\"all\").top(time_filter=\"hour\")\n",
       "\u001b[0;31mFile:\u001b[0m      ~/miniconda3/envs/sa/lib/python3.8/site-packages/praw/models/listing/mixins/base.py\n",
       "\u001b[0;31mType:\u001b[0m      method\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# try run this line, what do you see? press q once you are done\n",
    "?subreddit.top "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T13:20:04.767333Z",
     "iopub.status.busy": "2022-07-14T13:20:04.767126Z",
     "iopub.status.idle": "2022-07-14T13:20:15.041044Z",
     "shell.execute_reply": "2022-07-14T13:20:15.036588Z",
     "shell.execute_reply.started": "2022-07-14T13:20:04.767315Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Project] From books to presentations in 10s with AR + ML\n",
      "[D] A Demo from 1993 of 32-year-old Yann LeCun showing off the World's first Convolutional Network for Text Recognition\n",
      "[R] First Order Motion Model applied to animate paintings\n",
      "[N] AI can turn old photos into moving Images / Link is given in the comments - You can also turn your old photo like this\n",
      "[D] This AI reveals how much time politicians stare at their phone at work\n",
      "[D] Types of Machine Learning Papers\n",
      "[D] The machine learning community has a toxicity problem\n",
      "[Project] NEW PYTHON PACKAGE: Sync GAN Art to Music with \"Lucid Sonic Dreams\"! (Link in Comments)\n",
      "I made a robot that punishes me if it detects that if I am procrastinating on my assignments [P]\n",
      "[P] Using oil portraits and First Order Model to bring the paintings back to life\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "top10 = subreddit.top(limit=10)\n",
    "for submission in top10:\n",
    "    print(submission.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details> <summary>Expected output:</summary>\n",
    "\n",
    "    [Project] From books to presentations in 10s with AR + ML\n",
    "    [D] A Demo from 1993 of 32-year-old Yann LeCun showing off the World's first Convolutional Network for Text Recognition\n",
    "    [R] First Order Motion Model applied to animate paintings\n",
    "    [N] AI can turn old photos into moving Images / Link is given in the comments - You can also turn your old photo like this\n",
    "    [D] This AI reveals how much time politicians stare at their phone at work\n",
    "    [D] Types of Machine Learning Papers\n",
    "    [D] The machine learning community has a toxicity problem\n",
    "    [Project] NEW PYTHON PACKAGE: Sync GAN Art to Music with \"Lucid Sonic Dreams\"! (Link in Comments)\n",
    "    [P] Using oil portraits and First Order Model to bring the paintings back to life\n",
    "    [D] Convolution Neural Network Visualization - Made with Unity 3D and lots of Code / source - stefsietz (IG)    \n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 2. Top 10 Posts of This Week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What are the titles of the top 10 posts of **this week** from your favorite subreddit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T13:20:15.048545Z",
     "iopub.status.busy": "2022-07-14T13:20:15.047620Z",
     "iopub.status.idle": "2022-07-14T13:20:25.357360Z",
     "shell.execute_reply": "2022-07-14T13:20:25.354891Z",
     "shell.execute_reply.started": "2022-07-14T13:20:15.048465Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30% of Google's Reddit Emotions Dataset is Mislabeled [D]\n",
      "[D] An accusation of academic misconduct by Prof. Yisen Wang (Peking University) in ICML2021 and NeurIPS2021\n",
      "[R] mixed reality future ‚Äî see the world through artistic lenses ‚Äî made with NeRF\n",
      "[N] First-Ever Course on Transformers: NOW PUBLIC\n",
      "[D] Why are Corgi dogs so popular in machine learning (especially in the image generation community)?\n",
      "[D] Noam Chomsky on LLMs and discussion of LeCun paper (MLST)\n",
      "[P] Sioyek 1.4 | Academic PDF Viewer\n",
      "[R] So someone actually peer-reviewed this and thought \"yeah, looks good\"?\n",
      "[N] Andrej Karpathy is leaving Tesla\n",
      "[D] How do you verify the novelty of your research?\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "top10 = subreddit.top(time_filter='week', limit=10)\n",
    "for submission in top10:\n",
    "    print(submission.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<details><summary>Expected output:</summary>\n",
    "\n",
    "    [N] Ian Goodfellow, Apple‚Äôs director of machine learning, is leaving the company due to its return to work policy. In a note to staff, he said ‚ÄúI believe strongly that more flexibility would have been the best policy for my team.‚Äù He was likely the company‚Äôs most cited ML expert.\n",
    "    [R][P] Thin-Plate Spline Motion Model for Image Animation + Gradio Web Demo\n",
    "    [P] I‚Äôve been trying to understand the limits of some of the available machine learning models out there. Built an app that lets you try a mix of CLIP from Open AI + Apple‚Äôs version of MobileNet, and more directly on your phone's camera roll.\n",
    "    [R] Meta is releasing a 175B parameter language model\n",
    "    [N] Hugging Face raised $100M at $2B to double down on community, open-source & ethics\n",
    "    [P] T-SNE to view and order your Spotify tracks\n",
    "    [D] : HELP Finding a Book - A book written for Google Engineers about foundational Math to support ML\n",
    "    [R] Scaled up CLIP-like model (~2B) shows 86% Zero-shot on Imagenet\n",
    "    [D] Do you use NLTK or Spacy for text preprocessing?\n",
    "    [D] Democratizing Diffusion Models - LDMs: High-Resolution Image Synthesis with Latent Diffusion Models, a 5-minute paper summary by Casual GAN Papers\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíΩ‚ùì Data Question:\n",
    "\n",
    "Check out what other attributes the `praw.models.Submission` class has in the [docs](https://praw.readthedocs.io/en/stable/code_overview/models/submission.html). \n",
    "\n",
    "1. After having a chance to look through the docs, is there any other information that you might want to extract? How might this additional data help you?\n",
    "\n",
    "   * I might want to extract *author*, *name*, *comments* and *num_comments*, *score* and *upvote_ratio*.\n",
    "   * Last information could tell us more about popularity of the post.\n",
    "\n",
    "Write a sample piece of code below extracting three additional pieces of information from the submission below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T13:21:24.473875Z",
     "iopub.status.busy": "2022-07-14T13:21:24.473599Z",
     "iopub.status.idle": "2022-07-14T13:21:24.756074Z",
     "shell.execute_reply": "2022-07-14T13:21:24.755143Z",
     "shell.execute_reply.started": "2022-07-14T13:21:24.473855Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30% of Google's Reddit Emotions Dataset is Mislabeled [D] \n",
      "\t Comments: 111  Score: 668  Upvote Ratio: 0.98\n",
      "[D] An accusation of academic misconduct by Prof. Yisen Wang (Peking University) in ICML2021 and NeurIPS2021 \n",
      "\t Comments: 115  Score: 648  Upvote Ratio: 0.97\n",
      "[R] mixed reality future ‚Äî see the world through artistic lenses ‚Äî made with NeRF \n",
      "\t Comments: 15  Score: 356  Upvote Ratio: 0.96\n",
      "[N] First-Ever Course on Transformers: NOW PUBLIC \n",
      "\t Comments: 37  Score: 344  Upvote Ratio: 0.92\n",
      "[D] Why are Corgi dogs so popular in machine learning (especially in the image generation community)? \n",
      "\t Comments: 68  Score: 320  Upvote Ratio: 0.92\n",
      "[D] Noam Chomsky on LLMs and discussion of LeCun paper (MLST) \n",
      "\t Comments: 240  Score: 273  Upvote Ratio: 0.88\n",
      "[P] Sioyek 1.4 | Academic PDF Viewer \n",
      "\t Comments: 16  Score: 248  Upvote Ratio: 1.0\n",
      "[R] So someone actually peer-reviewed this and thought \"yeah, looks good\"? \n",
      "\t Comments: 77  Score: 227  Upvote Ratio: 0.96\n",
      "[N] Andrej Karpathy is leaving Tesla \n",
      "\t Comments: 81  Score: 187  Upvote Ratio: 0.98\n",
      "[D] How do you verify the novelty of your research? \n",
      "\t Comments: 55  Score: 184  Upvote Ratio: 0.99\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "top10 = subreddit.top(time_filter='week', limit=10)\n",
    "for submission in top10:\n",
    "    title = submission.title\n",
    "    comments = submission.num_comments\n",
    "    score = submission.score\n",
    "    upvote_ratio = submission.upvote_ratio\n",
    "    print(f\"{title} \\n\\t Comments: {comments}  Score: {score}  Upvote Ratio: {upvote_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíΩ‚ùì Data Question:\n",
    "\n",
    "2. Is there any information available that might be a concern when it comes to Ethical Data?\n",
    "  \n",
    "   * I think that *over_18* might be a concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 3. Comment Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Add comments to the code block below to describe what each line of the code does (Refer to [Obtain Comment Instances Section](https://praw.readthedocs.io/en/stable/getting_started/quick_start.html) when necessary). The code is adapted from [this tutorial](https://praw.readthedocs.io/en/stable/tutorials/comments.html)\n",
    "\n",
    "The purpose is \n",
    "1. to understand what the code is doing \n",
    "2. start to comment your code whenever it is not self-explantory if you have not (others will thank you, YOU will thank you later üòä) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T13:22:25.073863Z",
     "iopub.status.busy": "2022-07-14T13:22:25.073592Z",
     "iopub.status.idle": "2022-07-14T13:24:17.482070Z",
     "shell.execute_reply": "2022-07-14T13:24:17.481247Z",
     "shell.execute_reply.started": "2022-07-14T13:22:25.073843Z"
    },
    "hidden": true,
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 468 ms, sys: 36.4 ms, total: 505 ms\n",
      "Wall time: 1min 52s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from praw.models import MoreComments\n",
    "\n",
    "# YOUR COMMENT HERE\n",
    "# Initialize an empty list\n",
    "top_comments = []\n",
    "\n",
    "# YOUR COMMENT HERE\n",
    "# Iterate through top 10 posts\n",
    "for submission in subreddit.top(limit=10):\n",
    "    # YOUR COMMENT HERE\n",
    "    # Iterate through top-level comments in the thread\n",
    "    for top_level_comment in submission.comments:\n",
    "        # YOUR COMMENT HERE\n",
    "        # If a comment  is an instance of MoreComments, ignore it\n",
    "        if isinstance(top_level_comment, MoreComments):\n",
    "            continue\n",
    "        # YOUR COMMENT HERE\n",
    "        # Otherwise, append the list with comment's body\n",
    "        top_comments.append(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "#### 4. Inspect Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "How many comments did you extract from the last step? Examine a few comments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T13:24:46.072440Z",
     "iopub.status.busy": "2022-07-14T13:24:46.072096Z",
     "iopub.status.idle": "2022-07-14T13:24:46.083116Z",
     "shell.execute_reply": "2022-07-14T13:24:46.082228Z",
     "shell.execute_reply.started": "2022-07-14T13:24:46.072421Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "737"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#YOUR CODE HERE  # the answer may vary 693 for r/machinelearning\n",
    "len(top_comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T13:33:26.351031Z",
     "iopub.status.busy": "2022-07-14T13:33:26.350744Z",
     "iopub.status.idle": "2022-07-14T13:33:26.358599Z",
     "shell.execute_reply": "2022-07-14T13:33:26.357572Z",
     "shell.execute_reply.started": "2022-07-14T13:33:26.351012Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Take my money', 'Need to know the pr0n ratio', 'Wow.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "[random.choice(top_comments) for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details> <summary>Some of the comments from `r/machinelearning` subreddit are:</summary>\n",
    "\n",
    "    ['Awesome visualisation',\n",
    "    'Similar to a stack or connected neurons.',\n",
    "    'Will this Turing pass the Turing Test?']\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíΩ‚ùì Data Question:\n",
    "\n",
    "3. After having a chance to review a few samples of 5 comments from the subreddit, what can you say about the data? \n",
    "\n",
    "   * The data is not very clean. There are many acronyms, abbreviations or chat slang words (lol, plz, ccp, ...). Not sure how sentiment analysis models are good with these kind of words.\n",
    "   * I also noticed some incorrect word contractions like dont and cant.\n",
    "   * Our task is to do sentiment analysis of Tesla stocks, but in the data we can not see anything about Tesla or any other stocks.\n",
    "\n",
    "\n",
    "HINT: Think about the \"cleanliness\" of the data, the content of the data, think about what you're trying to do - how does this data line up with your goal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 5. Extract Top Level Comment from Subreddit `TSLA`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Write your code to extract top level comments from the top 10 topics of a time period, e.g., year, from subreddit `TSLA` and store them in a list `top_comments_tsla`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T13:44:19.339775Z",
     "iopub.status.busy": "2022-07-14T13:44:19.339374Z",
     "iopub.status.idle": "2022-07-14T13:45:12.679531Z",
     "shell.execute_reply": "2022-07-14T13:45:12.676382Z",
     "shell.execute_reply.started": "2022-07-14T13:44:19.339751Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Up Vote if you would like to see a 1for 5 stock split?\n",
      "Raise your hands if you stayed long and didn‚Äôt de-risk on Tesla because of your research and faith in the actual company.\n",
      "Vote up for a 10 for 1 stock split (at 1500/share before split)!\n",
      "I‚Äôm a small fry, but happy to say I‚Äôve just passed Half a share of TSLA! Hoping to get to a full share by year end.\n",
      "Record Deliveries in Q4 2021\n",
      "All in on TSLA\n",
      "Pepsi buying Tesla trucks\n",
      "Tesla To 1.2k and Beyond\n",
      "Bought on the dip today. Who‚Äôs with me?\n",
      "Congrats my fellow longs\n",
      "CPU times: user 338 ms, sys: 0 ns, total: 338 ms\n",
      "Wall time: 53.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# YOUR CODE HERE\n",
    "subreddit = reddit.subreddit(\"TSLA\")\n",
    "\n",
    "top10 = subreddit.top(time_filter='year', limit=10)\n",
    "for submission in top10:\n",
    "    print(submission.title)\n",
    "\n",
    "from praw.models import MoreComments\n",
    "\n",
    "# Initialize an empty list\n",
    "top_comments_tsla = []\n",
    "\n",
    "# Iterate through top 10 posts\n",
    "for submission in subreddit.top(limit=10):\n",
    "    # Iterate through top-level comments in the thread\n",
    "    for top_level_comment in submission.comments:\n",
    "        # If a comment  is an instance of MoreComments, ignore it\n",
    "        if isinstance(top_level_comment, MoreComments):\n",
    "            continue\n",
    "        # Otherwise, append the list with comment's body\n",
    "        top_comments_tsla.append(top_level_comment.body)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T13:46:16.766602Z",
     "iopub.status.busy": "2022-07-14T13:46:16.766134Z",
     "iopub.status.idle": "2022-07-14T13:46:16.772727Z",
     "shell.execute_reply": "2022-07-14T13:46:16.771614Z",
     "shell.execute_reply.started": "2022-07-14T13:46:16.766578Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "170"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(top_comments_tsla) # Expected: 174 for r/machinelearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T13:47:25.934419Z",
     "iopub.status.busy": "2022-07-14T13:47:25.932943Z",
     "iopub.status.idle": "2022-07-14T13:47:25.940185Z",
     "shell.execute_reply": "2022-07-14T13:47:25.939365Z",
     "shell.execute_reply.started": "2022-07-14T13:47:25.934398Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Base in uk, who knows best apps to buy and sell or FOR investors for TSLA.....Thank you guys!',\n",
       " 'What does a split mean? Is it bad for people that have shares?',\n",
       " 'Stacked up on Tesla and big oil 2 years ago. Can‚Äôt be happier.']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[random.choice(top_comments_tsla) for i in range(3)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Some of the comments from `r/TSLA` subreddit:</summary>\n",
    "\n",
    "    ['I bought puts',\n",
    "    '100%',\n",
    "    'Yes. And I‚Äôm bag holding 1200 calls for Friday and am close to throwing myself out the window']\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíΩ‚ùì Data Question:\n",
    "\n",
    "4. Now that you've had a chance to review another subreddits comments, do you see any differences in the kinds of comments either subreddit has - and how might this relate to bias?\n",
    "\n",
    "   * Comments from TSLA subreddit fit much better our task than comments from the machinelearning subreddit. \n",
    "   * Our population should consist from people who discuss and display their feelings about Tesla. So, we want to extract data from such communities. \n",
    "   * Maybe we are in a danger to fall in a selection bias here. But what a result we could get from communities that are not talking about Tesla? \n",
    "   * To avoid selection bias and availability bias too, we should include more communities which users are displaying sentiments toward Tesla.\n",
    "   * By choosing additional communities we shouild try to avoid other types of bias as well..\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Task III: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let us analyze the sentiment of comments scraped from `r/TSLA` using a pre-trained HuggingFace model to make the inference. Take a [Quick tour](https://huggingface.co/docs/transformers/quicktour). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 1. Import `pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T17:48:37.087050Z",
     "iopub.status.busy": "2022-07-14T17:48:37.079791Z",
     "iopub.status.idle": "2022-07-14T17:48:40.192781Z",
     "shell.execute_reply": "2022-07-14T17:48:40.191795Z",
     "shell.execute_reply.started": "2022-07-14T17:48:37.086829Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 2. Create a Pipeline to Perform Task \"sentiment-analysis\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T17:55:01.229805Z",
     "iopub.status.busy": "2022-07-14T17:55:01.229475Z",
     "iopub.status.idle": "2022-07-14T17:55:05.311665Z",
     "shell.execute_reply": "2022-07-14T17:55:05.310848Z",
     "shell.execute_reply.started": "2022-07-14T17:55:01.229782Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    }
   ],
   "source": [
    "sentiment_model = pipeline(\"sentiment-analysis\") # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 3. Get one comment from list `top_comments_tsla` from Task II - 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T18:06:24.689725Z",
     "iopub.status.busy": "2022-07-14T18:06:24.689297Z",
     "iopub.status.idle": "2022-07-14T18:06:24.694283Z",
     "shell.execute_reply": "2022-07-14T18:06:24.693127Z",
     "shell.execute_reply.started": "2022-07-14T18:06:24.689697Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(11)\n",
    "comment = random.choice(top_comments_tsla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T18:06:26.177043Z",
     "iopub.status.busy": "2022-07-14T18:06:26.176783Z",
     "iopub.status.idle": "2022-07-14T18:06:26.182610Z",
     "shell.execute_reply": "2022-07-14T18:06:26.181652Z",
     "shell.execute_reply.started": "2022-07-14T18:06:26.177025Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"$580 and falling today. What's your price target?\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The example comment is: `'Bury Burry!!!!!'`. Print out what you get. For reproducibility, use the same comment in the next step; consider setting a seed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### 4. Make Inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T18:13:54.621455Z",
     "iopub.status.busy": "2022-07-14T18:13:54.621202Z",
     "iopub.status.idle": "2022-07-14T18:13:54.649407Z",
     "shell.execute_reply": "2022-07-14T18:13:54.648368Z",
     "shell.execute_reply.started": "2022-07-14T18:13:54.621436Z"
    },
    "hidden": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'label': 'NEGATIVE', 'score': 0.9976972937583923}]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment = sentiment_model(comment)  # YOUR CODE HERE \n",
    "print(type(sentiment))\n",
    "sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "What is the type of the output `sentiment`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "```\n",
    "YOUR ANSWER HERE\n",
    "\n",
    "```\n",
    "   * It is a list of dictionaries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T18:09:26.782883Z",
     "iopub.status.busy": "2022-07-14T18:09:26.782552Z",
     "iopub.status.idle": "2022-07-14T18:09:26.789432Z",
     "shell.execute_reply": "2022-07-14T18:09:26.788129Z",
     "shell.execute_reply.started": "2022-07-14T18:09:26.782860Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The comment: $580 and falling today. What's your price target?\n",
      "Predicted Label is NEGATIVE and the score is 0.998\n"
     ]
    }
   ],
   "source": [
    "print(f'The comment: {comment}')\n",
    "print(f'Predicted Label is {sentiment[0][\"label\"]} and the score is {sentiment[0][\"score\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the example comment, the output is:\n",
    "\n",
    "    The comment: Bury Burry!!!!!\n",
    "    Predicted Label is NEGATIVE and the score is 0.989"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üñ•Ô∏è‚ùì Model Question:\n",
    "\n",
    "1. What does the score represent?\n",
    "   * The score represents probability for the predicted label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task IV: Put All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pull all the piece together, create a simple script that does \n",
    "\n",
    "- get the subreddit\n",
    "- get comments from the top posts for given subreddit\n",
    "- run sentiment analysis "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Complete the Script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you complete the code, running the following block writes the code into a new Python script and saves it as `top_tlsa_comment_sentiment.py` under the same directory with the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "code_folding": [],
    "execution": {
     "iopub.execute_input": "2022-07-14T19:32:36.333000Z",
     "iopub.status.busy": "2022-07-14T19:32:36.332396Z",
     "iopub.status.idle": "2022-07-14T19:32:36.356811Z",
     "shell.execute_reply": "2022-07-14T19:32:36.355796Z",
     "shell.execute_reply.started": "2022-07-14T19:32:36.332943Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing top_tlsa_comment_sentiment.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile top_tlsa_comment_sentiment.py\n",
    "\n",
    "import secrets_reddit\n",
    "import random\n",
    "\n",
    "from typing import Dict, List\n",
    "\n",
    "from praw import Reddit\n",
    "from praw.models.reddit.subreddit import Subreddit\n",
    "from praw.models import MoreComments\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def get_subreddit(display_name:str) -> Subreddit:\n",
    "    \"\"\"Get subreddit object from display name\n",
    "\n",
    "    Args:\n",
    "        display_name (str): Name of subreddit (community)\n",
    "\n",
    "    Returns:\n",
    "        Subreddit: Subreddit object\n",
    "    \"\"\"\n",
    "    reddit = Reddit(\n",
    "        client_id=secrets_reddit.REDDIT_API_CLIENT_ID,        \n",
    "        client_secret=secrets_reddit.REDDIT_API_CLIENT_SECRET,\n",
    "        user_agent=secrets_reddit.REDDIT_API_USER_AGENT\n",
    "        )\n",
    "    \n",
    "    subreddit = reddit.subreddit(display_name) # YOUR CODE HERE\n",
    "    return subreddit\n",
    "\n",
    "def get_comments(subreddit:Subreddit, limit:int=3) -> List[str]:\n",
    "    \"\"\" Get comments from subreddit\n",
    "\n",
    "    Args:\n",
    "        subreddit (Subreddit): Subreddit object\n",
    "        limit (int, optional): Number of posts (submissions) to extract. Defaults to 3.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: List of comments\n",
    "    \"\"\"\n",
    "    top_comments = []\n",
    "    for submission in subreddit.top(limit=limit):\n",
    "        for top_level_comment in submission.comments:\n",
    "            if isinstance(top_level_comment, MoreComments):\n",
    "                continue\n",
    "            top_comments.append(top_level_comment.body)\n",
    "    return top_comments\n",
    "\n",
    "def run_sentiment_analysis(comment:str) -> Dict:\n",
    "    \"\"\"Run sentiment analysis on comment using default distilbert model\n",
    "    \n",
    "    Args:\n",
    "        comment (str): Comment\n",
    "        \n",
    "    Returns:\n",
    "        str: Sentiment analysis result\n",
    "    \"\"\"\n",
    "    sentiment_model = pipeline(\"sentiment-analysis\")  # YOUR CODE HERE\n",
    "    sentiment = sentiment_model(comment)\n",
    "    return sentiment[0]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    subreddit = get_subreddit(\"TSLA\")  # YOUR CODE HERE\n",
    "    comments = get_comments(subreddit)\n",
    "    comment = random.choice(comments)  # YOUR CODE HERE\n",
    "    sentiment = run_sentiment_analysis(comment)\n",
    "    \n",
    "    print(f'The comment: {comment}')\n",
    "    print(f'Predicted Label is {sentiment[\"label\"]} and the score is {sentiment[\"score\"]:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following block to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-14T19:44:06.046227Z",
     "iopub.status.busy": "2022-07-14T19:44:06.045853Z",
     "iopub.status.idle": "2022-07-14T19:44:42.410977Z",
     "shell.execute_reply": "2022-07-14T19:44:42.405486Z",
     "shell.execute_reply.started": "2022-07-14T19:44:06.046204Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
      "The comment: You did - nicely done !!!\n",
      "Predicted Label is POSITIVE and the score is 1.000\n"
     ]
    }
   ],
   "source": [
    "# To disable tokenizers parallelism warning\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "!python top_tlsa_comment_sentiment.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details><summary> Expected output:</summary>\n",
    "\n",
    "    No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n",
    "    The comment: When is DOGE flying\n",
    "    Predicted Label is POSITIVE and the score is 0.689\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíΩ‚ùì Data Question:\n",
    "\n",
    "5. Is the subreddit active? About how many posts or threads per day? How could you find this information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "     \n",
    "  * This subreddit is active. We can list creation dates of newest posts or comments to confirm that.\n",
    "  * Distribution of post per day was analyzed below Data Question 6.\n",
    "    * In almost 96% of days there were 1 - 6 posts per day\n",
    "    * 1 post was in about 43% of days and 2 posts in 23%\n",
    "   * To get creation dates for posts I used the Submission object with the argument *created*\n",
    "   * Counter collection was used twice to get distribution of posts per days\n",
    "     1. count number of posts per day\n",
    "     1. count number of days with particular number of postings\n",
    "   * Similar analysis could be done with Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T14:53:11.197514Z",
     "iopub.status.busy": "2022-07-15T14:53:11.196330Z",
     "iopub.status.idle": "2022-07-15T14:53:11.690820Z",
     "shell.execute_reply": "2022-07-15T14:53:11.689663Z",
     "shell.execute_reply.started": "2022-07-15T14:53:11.197477Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-15 00:05:44\n",
      "2022-07-14 04:21:43\n",
      "2022-07-13 12:42:15\n",
      "2022-07-13 11:17:53\n",
      "2022-07-13 11:13:47\n"
     ]
    }
   ],
   "source": [
    "# import datetime\n",
    "from datetime import datetime\n",
    "\n",
    "# List creation dates for 5 newest posts\n",
    "posts = subreddit.new( limit=5)\n",
    "for submission in posts:\n",
    "    print(datetime.fromtimestamp(submission.created))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T13:37:17.968236Z",
     "iopub.status.busy": "2022-07-15T13:37:17.967784Z",
     "iopub.status.idle": "2022-07-15T13:37:18.085842Z",
     "shell.execute_reply": "2022-07-15T13:37:18.084771Z",
     "shell.execute_reply.started": "2022-07-15T13:37:17.968212Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-07-15 00:07:17\n",
      "2022-07-15 00:05:54\n",
      "2022-07-14 19:09:05\n",
      "2022-07-14 11:14:57\n",
      "2022-07-14 11:12:23\n"
     ]
    }
   ],
   "source": [
    "# List creation dates for 5 newest comments\n",
    "comments = subreddit.comments( limit=5)\n",
    "for comment in comments:\n",
    "    print(datetime.fromtimestamp(comment.created))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üíΩ‚ùì Data Question:\n",
    "\n",
    "6. Does there seem to be a large distribution of posters or a smaller concentration of posters who are very active? What kind of impact might this have on the data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  * There is a large distribution of posters\n",
    "    * Percentage of posters with 1 or 2 posts is about 87%.\n",
    "    * There are 3 posters (of 482) with more than 20 posts\n",
    "   * This is good for data because it is not impacted by a small group of posters. That helps to avoid selection bias."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Collecting authors and creation dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T15:00:51.706692Z",
     "iopub.status.busy": "2022-07-15T15:00:51.705474Z",
     "iopub.status.idle": "2022-07-15T15:02:35.024554Z",
     "shell.execute_reply": "2022-07-15T15:02:35.022765Z",
     "shell.execute_reply.started": "2022-07-15T15:00:51.706669Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collect information about authors and creation dates for every post in subreddit\n",
    "authors = []\n",
    "date_created = []\n",
    "\n",
    "posts = subreddit.new( limit=None)\n",
    "# Iteratr through all posts\n",
    "for submission in posts:\n",
    "    # To avoid issues with authors eqal None\n",
    "    if submission.author is not None:\n",
    "        authors.append(submission.author.name)\n",
    "        date_created.append(datetime.fromtimestamp(submission.created).strftime('%Y-%m-%d'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T15:04:35.126260Z",
     "iopub.status.busy": "2022-07-15T15:04:35.125954Z",
     "iopub.status.idle": "2022-07-15T15:04:35.133530Z",
     "shell.execute_reply": "2022-07-15T15:04:35.132427Z",
     "shell.execute_reply.started": "2022-07-15T15:04:35.126239Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "976\n",
      "['wewewawa', 'AdmirableActuator', 'StocksWithCamden', 'ju-ju2020']\n",
      "976\n",
      "['2022-07-15', '2022-07-14', '2022-07-13', '2022-07-13']\n"
     ]
    }
   ],
   "source": [
    "print(len(authors))\n",
    "print(authors[:4])\n",
    "print(len(date_created))\n",
    "print(date_created[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submissions per days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T16:07:38.352046Z",
     "iopub.status.busy": "2022-07-15T16:07:38.351130Z",
     "iopub.status.idle": "2022-07-15T16:07:38.361122Z",
     "shell.execute_reply": "2022-07-15T16:07:38.360241Z",
     "shell.execute_reply.started": "2022-07-15T16:07:38.352026Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('2022-07-15', 1), ('2022-07-14', 1), ('2022-07-13', 4), ('2022-07-11', 1)]\n",
      "\n",
      " [(1, 164), (2, 87), (3, 45), (4, 37), (5, 17), (6, 12), (11, 3), (7, 3), (8, 3), (13, 2), (17, 2), (12, 1), (33, 1), (15, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Import Counter\n",
    "from collections import Counter\n",
    "# Count number of posts per dates\n",
    "n_posts = Counter(date_created)\n",
    "print(list(n_posts.items())[:4])\n",
    "\n",
    "# Count frequency of postings and sort \n",
    "freq = Counter(n_posts.values()).most_common()\n",
    "print('\\n', freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 164 days we have only one posting, 87 days 2 postings and so on. Let's calculate percentage of days with 1 - 6 postings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T16:07:49.099083Z",
     "iopub.status.busy": "2022-07-15T16:07:49.097673Z",
     "iopub.status.idle": "2022-07-15T16:07:49.105812Z",
     "shell.execute_reply": "2022-07-15T16:07:49.104753Z",
     "shell.execute_reply.started": "2022-07-15T16:07:49.099062Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of days with 1 - 6 postings per day: 362\n",
      "Total number of days: 378\n",
      "Percentage of days with 1 - 6 postings: 95.77%\n",
      "Percentage of days with 1 posting: 43.39%\n",
      "Percentage of days with 2 postings: 23.02%\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of days with 1 - 6 postings\n",
    "n = 6  \n",
    "s = 0\n",
    "for i in range(n):\n",
    "    s += freq[i][1]\n",
    "print(f\"Total number of days with 1 - 6 postings per day: {s}\")\n",
    "\n",
    "# Total number of days \n",
    "d = len(set(date_created))\n",
    "print(f\"Total number of days: {d}\")\n",
    "print(f\"Percentage of days with 1 - 6 postings: {round(100 * (s / d), 2)}%\")\n",
    "print(f\"Percentage of days with 1 posting: {round(100 * (freq[0][1]/ d), 2)}%\")\n",
    "print(f\"Percentage of days with 2 postings: {round(100 * (freq[1][1] / d), 2)}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submissions per authors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T16:08:44.451055Z",
     "iopub.status.busy": "2022-07-15T16:08:44.450738Z",
     "iopub.status.idle": "2022-07-15T16:08:44.460837Z",
     "shell.execute_reply": "2022-07-15T16:08:44.459786Z",
     "shell.execute_reply.started": "2022-07-15T16:08:44.451036Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('wewewawa', 52), ('AdmirableActuator', 1), ('StocksWithCamden', 1), ('ju-ju2020', 2)]\n",
      "\n",
      " [(1, 348), (2, 71), (3, 22), (6, 10), (5, 6), (9, 5), (4, 5), (8, 2), (11, 2), (14, 2), (52, 1), (51, 1), (13, 1), (12, 1), (7, 1), (16, 1), (10, 1), (23, 1), (15, 1)]\n"
     ]
    }
   ],
   "source": [
    "# Number of posts per authors\n",
    "n_authors = Counter(authors)\n",
    "print(list(n_authors.items())[:4])\n",
    "\n",
    "# Count frequency of postings and sort \n",
    "afreq = Counter(n_authors.values()).most_common()\n",
    "print('\\n', afreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-15T16:30:52.875019Z",
     "iopub.status.busy": "2022-07-15T16:30:52.874660Z",
     "iopub.status.idle": "2022-07-15T16:30:52.882020Z",
     "shell.execute_reply": "2022-07-15T16:30:52.881208Z",
     "shell.execute_reply.started": "2022-07-15T16:30:52.874998Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of authors with 1 - 2 postings per day: 419\n",
      "Number of authors with more than 20 postings per day: 3\n",
      "Total number of authors: 482\n",
      "Percentage of authors with 1 - 2 postings: 86.93%\n"
     ]
    }
   ],
   "source": [
    "# Calculate number of authors with 1 - 2 postings\n",
    "n = 2 \n",
    "nl = 20\n",
    "s = 0\n",
    "sl = 0\n",
    "for i in range(len(afreq)):\n",
    "    # Calculate number of authors with only few posts\n",
    "    if afreq[i][0] <= 2:\n",
    "        s += afreq[i][1]\n",
    "    # Calculate number of authors with many posts       \n",
    "    elif afreq[i][0] >= nl:\n",
    "        sl += afreq[i][1]\n",
    "print(f\"Number of authors with 1 - 2 postings per day: {s}\")\n",
    "print(f\"Number of authors with more than {nl} postings per day: {sl}\")\n",
    "\n",
    "# Total number of authors \n",
    "a = len(set(authors))\n",
    "print(f\"Total number of authors: {a}\")\n",
    "print(f\"Percentage of authors with 1 - 2 postings: {round(100 * (s / a), 2)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sa",
   "language": "python",
   "name": "conda-env-sa-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "vscode": {
   "interpreter": {
    "hash": "c57794392b841cffd8686d5c4548e4e2ec78521f49300d60954d1380f1b4bd1f"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
